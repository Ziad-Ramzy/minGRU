# minGRU Implementation

## Overview

This repository contains a minimalist implementation of a Gated Recurrent Unit (GRU) model in PyTorch. The purpose of this implementation is to provide a clear and concise understanding of the GRU architecture, focusing on its key components and functionalities, such as batch normalization and parameter optimization.

## Features

- **Simple GRU Model**: A straightforward implementation of the GRU architecture.
- **Batch Normalization**: Incorporates batch normalization to stabilize and accelerate training.
- **Custom Loss Function**: Utilizes Cross Entropy Loss for training and evaluation.
- **Modular Design**: Code is organized in a way that allows for easy modification and experimentation.
